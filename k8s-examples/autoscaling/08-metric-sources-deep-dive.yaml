# Deep Dive: Where Do HPA and VPA Metrics Actually Come From?
# WHY: Understanding the data pipeline helps you troubleshoot and optimize
# WHAT: The complete journey from pod to autoscaling decision

# =================== THE METRIC PIPELINE ===================
# 
# 1. KUBELET (on each node) collects container metrics
#    ↓
# 2. METRICS-SERVER aggregates across cluster
#    ↓  
# 3. HPA Controller queries metrics-server via API
#    ↓
# 4. HPA makes scaling decisions
# 
# Meanwhile, in parallel:
# 
# 1. KUBELET collects historical usage data
#    ↓
# 2. VPA RECOMMENDER analyzes patterns
#    ↓
# 3. VPA provides resource recommendations
#    ↓
# 4. VPA UPDATER modifies pod specs (if enabled)

# =================== UNDERSTANDING THE DATA SOURCES ===================

# Example deployment to understand metric sources
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metric-source-demo
spec:
  replicas: 3
  selector:
    matchLabels:
      app: metric-source-demo
  template:
    metadata:
      labels:
        app: metric-source-demo
    spec:
      containers:
      - name: app
        image: nginx:1.21
        resources:
          requests:
            cpu: 100m        # HPA baseline for calculations
            memory: 128Mi    # HPA baseline for calculations
          limits:
            cpu: 500m        # NOT used by HPA calculations
            memory: 512Mi    # NOT used by HPA calculations

---
# =================== HPA METRIC CALCULATION EXAMPLES ===================

# Example 1: CPU Request vs Actual Usage
# 
# Pod Configuration:
# requests:
#   cpu: 100m    # 0.1 CPU cores requested
# 
# Scenario A: Light load
# - Actual CPU usage: 30m
# - HPA CPU utilization: (30m / 100m) × 100 = 30%
# - Decision: Scale down if current replicas > minReplicas
# 
# Scenario B: Heavy load  
# - Actual CPU usage: 80m
# - HPA CPU utilization: (80m / 100m) × 100 = 80%
# - Decision: Scale up if 80% > target (70%)
# 
# Scenario C: Overloaded
# - Actual CPU usage: 120m
# - HPA CPU utilization: (120m / 100m) × 100 = 120%
# - Decision: Scale up aggressively

---
# =================== VPA METRIC ANALYSIS ===================

# VPA analyzes different data points:
# 
# 1. Historical CPU Usage Pattern:
#    Day 1: [45m, 60m, 55m, 70m, 80m]
#    Day 2: [50m, 65m, 60m, 75m, 85m]
#    Day 3: [48m, 62m, 58m, 72m, 82m]
# 
# 2. VPA Statistical Analysis:
#    - 50th percentile (median): ~62m
#    - 90th percentile: ~82m
#    - 95th percentile: ~85m
#    - Peak usage: 85m
# 
# 3. VPA Recommendation Logic:
#    - Target: 90th percentile + 15% buffer = ~95m
#    - Lower bound: 50th percentile = ~62m
#    - Upper bound: 95th percentile + 25% buffer = ~106m
# 
# 4. VPA Recommendation:
#    requests:
#      cpu: 95m     # Optimized based on actual usage
#      memory: 180Mi # Based on memory usage patterns

---
# =================== METRICS API ENDPOINTS ===================
# 
# HPA queries these APIs:
# 
# 1. Resource Metrics (CPU, Memory):
# GET /apis/metrics.k8s.io/v1beta1/namespaces/default/pods
# 
# Response example:
# {
#   "items": [
#     {
#       "metadata": {"name": "pod-1"},
#       "containers": [
#         {
#           "name": "app",
#           "usage": {
#             "cpu": "75m",      # Current CPU usage
#             "memory": "150Mi"  # Current memory usage
#           }
#         }
#       ]
#     }
#   ]
# }
# 
# 2. Custom Metrics (via custom metrics API):
# GET /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests_per_second
# 
# 3. External Metrics (via external metrics API):
# GET /apis/external.metrics.k8s.io/v1beta1/namespaces/default/queue_length

---
# =================== PRACTICAL DEBUGGING COMMANDS ===================

# Debug HPA metrics:
# 
# 1. Check if metrics-server is running:
# kubectl get pods -n kube-system | grep metrics-server
# 
# 2. Test metrics API directly:
# kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes
# kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods
# 
# 3. Check HPA calculations:
# kubectl describe hpa <hpa-name>
# 
# Look for:
# Metrics:                               ( current / target )
#   resource cpu on pods  (as a percentage of request):  45% (45m) / 70%
#                                                         ↑    ↑     ↑
#                                                   percentage actual target
# 
# 4. See what HPA controller sees:
# kubectl get hpa <hpa-name> -o yaml
# 
# Look for:
# status:
#   currentMetrics:
#   - resource:
#       current:
#         averageUtilization: 45    # This is (actual/request) × 100
#         averageValue: 45m         # This is the actual usage
#       name: cpu
#     type: Resource

---
# =================== COMMON MISCONCEPTIONS ===================
# 
# ❌ WRONG: "HPA scales based on node CPU usage"
# ✅ RIGHT: "HPA scales based on pod CPU usage as % of requests"
# 
# ❌ WRONG: "70% CPU utilization means 70% of CPU limit"
# ✅ RIGHT: "70% CPU utilization means 70% of CPU request"
# 
# ❌ WRONG: "VPA uses the same metrics as HPA"
# ✅ RIGHT: "VPA analyzes historical actual usage patterns"
# 
# ❌ WRONG: "If I set CPU request = limit, HPA won't work"
# ✅ RIGHT: "HPA works fine, but pods can't use more than request"
# 
# =================== THE BOTTOM LINE ===================
# 
# HPA Metrics:
# - Source: metrics-server (current usage)
# - Calculation: (current usage / resource request) × 100
# - Frequency: Every 15 seconds (default)
# - Scope: Current moment snapshot
# 
# VPA Metrics:
# - Source: kubelet (historical usage)
# - Calculation: Statistical analysis of usage patterns
# - Frequency: Continuous collection, recommendations every few minutes
# - Scope: Historical trends and patterns
# 
# This is why resource REQUESTS are absolutely critical:
# - HPA uses them as the baseline for calculations
# - VPA uses them as the starting point for optimization
# - Both fail without proper requests configured!