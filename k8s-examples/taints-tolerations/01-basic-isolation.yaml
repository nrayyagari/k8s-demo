# Basic Taints and Tolerations: Node Access Control
# WHY: Prevent pods from running on inappropriate nodes
# PATTERN: Node restricts access â†’ Pod proves compatibility

---
# Scenario: Dedicated database node with high-memory requirements
# Step 1: Taint the node (run this command first)
#   kubectl taint nodes <node-name> workload=database:NoSchedule

---
# Pod WITHOUT toleration - REJECTED by tainted nodes
apiVersion: v1
kind: Pod
metadata:
  name: regular-web-app
  labels:
    app: webapp
    tier: frontend
spec:
  containers:
  - name: nginx
    image: nginx:1.21
    resources:
      requests:
        memory: "64Mi"
        cpu: "50m"
      limits:
        memory: "128Mi"
        cpu: "100m"
    ports:
    - containerPort: 80
  # NO tolerations = Cannot schedule on tainted nodes
  # This pod will be pending or go to untainted nodes only

---
# Pod WITH matching toleration - ACCEPTED by tainted nodes
apiVersion: v1
kind: Pod
metadata:
  name: postgres-database
  labels:
    app: database
    workload: database
spec:
  # Toleration matches the node taint exactly
  tolerations:
  - key: workload              # Must match taint key
    operator: Equal            # Exact value match required
    value: database           # Must match taint value
    effect: NoSchedule        # Must match taint effect
  containers:
  - name: postgres
    image: postgres:13
    env:
    - name: POSTGRES_DB
      value: "production"
    - name: POSTGRES_USER
      value: "dbuser"
    - name: POSTGRES_PASSWORD
      value: "secure-password-123"
    resources:
      requests:
        memory: "512Mi"      # High memory for database
        cpu: "200m"
      limits:
        memory: "1Gi"
        cpu: "500m"
    ports:
    - containerPort: 5432
    volumeMounts:
    - name: postgres-data
      mountPath: /var/lib/postgresql/data
  volumes:
  - name: postgres-data
    emptyDir: {}

---
# Pod with FLEXIBLE toleration - can run on multiple node types
apiVersion: v1
kind: Pod
metadata:
  name: monitoring-agent
  labels:
    app: monitoring
    type: system
spec:
  # Multiple tolerations for different scenarios
  tolerations:
  # Can run on database nodes
  - key: workload
    operator: Equal
    value: database
    effect: NoSchedule
  # Can run on nodes being prepared for maintenance
  - key: maintenance
    operator: Exists          # Any maintenance=* taint
    effect: NoSchedule
  # Can run on nodes that are temporarily unavailable
  - key: node.kubernetes.io/not-ready
    operator: Exists
    effect: NoSchedule
    tolerationSeconds: 60     # Wait 60 seconds before giving up
  containers:
  - name: prometheus-node-exporter
    image: prom/node-exporter:v1.6.0
    args:
    - '--path.rootfs=/host'
    resources:
      requests:
        memory: "32Mi"
        cpu: "25m"
      limits:
        memory: "64Mi"
        cpu: "50m"
    ports:
    - containerPort: 9100
    volumeMounts:
    - name: host-root
      mountPath: /host
      readOnly: true
  volumes:
  - name: host-root
    hostPath:
      path: /

---
# Testing Commands and Expected Behavior:

# 1. BEFORE applying this file:
#    kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints
#    (Should show minimal or no custom taints)

# 2. Apply the pods:
#    kubectl apply -f 01-basic-isolation.yaml

# 3. Check initial placement:
#    kubectl get pods -o wide
#    (All three pods should schedule normally)

# 4. NOW taint a node for database workloads:
#    kubectl taint nodes <node-name> workload=database:NoSchedule

# 5. Delete and recreate pods to test taint effect:
#    kubectl delete pod regular-web-app postgres-database monitoring-agent
#    kubectl apply -f 01-basic-isolation.yaml

# 6. Check new placement:
#    kubectl get pods -o wide
#    Expected results:
#    - regular-web-app: Pending OR on untainted nodes only
#    - postgres-database: Can schedule on tainted node
#    - monitoring-agent: Can schedule on tainted node

# 7. Check scheduling events:
#    kubectl describe pod regular-web-app | grep -A 5 Events
#    (Should show FailedScheduling due to taint)

# 8. Verify taint is working:
#    kubectl describe node <tainted-node> | grep -i taint
#    (Should show: workload=database:NoSchedule)

# 9. Clean up taint when done:
#    kubectl taint nodes <node-name> workload=database:NoSchedule-