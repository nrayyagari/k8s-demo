# Basic A/B Testing Example
# Purpose: Learn how to run controlled experiments to test different versions

# WHY: Need to measure impact of changes on user behavior and business metrics
# PROBLEM: Can't determine if new features improve or hurt key performance indicators
# SOLUTION: Split users into control/treatment groups and measure statistical differences

---
# CONTROL GROUP DEPLOYMENT (Version A - Current)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-control
  labels:
    app: webapp
    experiment: homepage-redesign
    variant: control
spec:
  replicas: 5                    # 50% of traffic (control group)
  selector:
    matchLabels:
      app: webapp
      variant: control
  template:
    metadata:
      labels:
        app: webapp
        variant: control
        experiment: homepage-redesign
    spec:
      containers:
      - name: webapp
        image: nginx:1.27-alpine
        ports:
        - containerPort: 8080
        
        # Control group configuration
        env:
        - name: APP_VERSION
          value: "v1.0.0"
        - name: EXPERIMENT_NAME
          value: "homepage-redesign"
        - name: VARIANT
          value: "control"
        - name: FEATURE_NEW_HOMEPAGE
          value: "false"          # Current homepage
        - name: BUTTON_COLOR
          value: "blue"           # Current button color
        - name: LAYOUT_TYPE
          value: "classic"        # Current layout
        
        # Analytics and tracking
        - name: ANALYTICS_ENDPOINT
          value: "https://analytics.company.com/events"
        - name: EXPERIMENT_ID
          value: "exp-homepage-001"
        
        # Health checks
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
        
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"

---
# TREATMENT GROUP DEPLOYMENT (Version B - New)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-treatment
  labels:
    app: webapp
    experiment: homepage-redesign
    variant: treatment
spec:
  replicas: 5                    # 50% of traffic (treatment group)
  selector:
    matchLabels:
      app: webapp
      variant: treatment
  template:
    metadata:
      labels:
        app: webapp
        variant: treatment
        experiment: homepage-redesign
    spec:
      containers:
      - name: webapp
        image: nginx:1.27-alpine
        ports:
        - containerPort: 8080
        
        # Treatment group configuration
        env:
        - name: APP_VERSION
          value: "v1.0.0"
        - name: EXPERIMENT_NAME
          value: "homepage-redesign"
        - name: VARIANT
          value: "treatment"
        - name: FEATURE_NEW_HOMEPAGE
          value: "true"           # New homepage design
        - name: BUTTON_COLOR
          value: "green"          # New button color
        - name: LAYOUT_TYPE
          value: "modern"         # New layout
        
        # Same analytics configuration
        - name: ANALYTICS_ENDPOINT
          value: "https://analytics.company.com/events"
        - name: EXPERIMENT_ID
          value: "exp-homepage-001"
        
        # Identical health checks
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
        
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        
        # Identical resource allocation
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"

---
# SERVICE (Routes to both control and treatment)
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  labels:
    app: webapp
    experiment: homepage-redesign
spec:
  type: ClusterIP
  selector:
    app: webapp                  # Routes to both variants
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP

---
# CONTROL GROUP SERVICE (Direct access for testing)
apiVersion: v1
kind: Service
metadata:
  name: webapp-control-service
  labels:
    app: webapp
    variant: control
spec:
  type: ClusterIP
  selector:
    app: webapp
    variant: control             # Only control group
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP

---
# TREATMENT GROUP SERVICE (Direct access for testing)
apiVersion: v1
kind: Service
metadata:
  name: webapp-treatment-service
  labels:
    app: webapp
    variant: treatment
spec:
  type: ClusterIP
  selector:
    app: webapp
    variant: treatment           # Only treatment group
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP

---
# CONFIGMAP FOR EXPERIMENT CONFIGURATION
apiVersion: v1
kind: ConfigMap
metadata:
  name: ab-test-config
  labels:
    experiment: homepage-redesign
data:
  experiment.json: |
    {
      "experiment_id": "exp-homepage-001",
      "name": "homepage-redesign",
      "description": "Test impact of new homepage design on conversion rate",
      "start_date": "2025-08-01T00:00:00Z",
      "end_date": "2025-08-15T23:59:59Z",
      "traffic_allocation": {
        "control": 0.5,
        "treatment": 0.5
      },
      "success_metrics": [
        {
          "name": "conversion_rate",
          "description": "Percentage of visitors who sign up",
          "type": "conversion",
          "goal": "increase"
        },
        {
          "name": "click_through_rate",
          "description": "Percentage who click main CTA button",
          "type": "engagement",
          "goal": "increase"
        },
        {
          "name": "bounce_rate",
          "description": "Percentage who leave without interaction",
          "type": "engagement",
          "goal": "decrease"
        }
      ],
      "guardrail_metrics": [
        {
          "name": "page_load_time",
          "threshold": 2000,
          "type": "performance"
        },
        {
          "name": "error_rate",
          "threshold": 0.01,
          "type": "reliability"
        }
      ],
      "statistical_config": {
        "confidence_level": 0.95,
        "power": 0.8,
        "minimum_detectable_effect": 0.05,
        "minimum_sample_size": 1000
      }
    }

---
# INGRESS WITH USER ASSIGNMENT LOGIC
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: webapp-ab-test-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/configuration-snippet: |
      # User assignment based on user ID hash
      set $user_id $http_x_user_id;
      set $variant "control";
      if ($user_id) {
        set_by_lua_block $variant {
          local user_id = ngx.var.user_id
          if user_id then
            local hash = ngx.crc32_long(user_id)
            if hash % 2 == 0 then
              return "control"
            else
              return "treatment"
            end
          end
          return "control"
        }
      }
      proxy_set_header X-Experiment-Variant $variant;
spec:
  ingressClassName: nginx
  rules:
  - host: myapp.company.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: webapp-service
            port:
              number: 80

# A/B TESTING EXPERIMENT DESIGN:
#
# HYPOTHESIS:
# "The new homepage design with green CTA buttons and modern layout
#  will increase conversion rate by at least 5% compared to the current design"
#
# EXPERIMENT SETUP:
# - Control Group (A): Current homepage (blue buttons, classic layout)
# - Treatment Group (B): New homepage (green buttons, modern layout)
# - Traffic Split: 50/50 between control and treatment
# - Duration: 2 weeks (sufficient for statistical significance)
# - Sample Size: Target 10,000 users per group
#
# SUCCESS METRICS:
# 1. Primary: Conversion Rate (sign-ups / unique visitors)
# 2. Secondary: Click-through Rate (CTA clicks / page views)
# 3. Secondary: Time on Page (engagement metric)
#
# GUARDRAIL METRICS:
# 1. Page Load Time (must not increase >10%)
# 2. Error Rate (must stay <1%)
# 3. Bounce Rate (should not increase significantly)

# USER ASSIGNMENT STRATEGY:
#
# CONSISTENT ASSIGNMENT (Recommended):
# - Hash user ID to determine group assignment
# - Same users always see same variant
# - Prevents user confusion from switching
# - Enables proper measurement of user journey
#
# COOKIE-BASED ASSIGNMENT:
# set $experiment_cookie $cookie_experiment_variant;
# if ($experiment_cookie = "") {
#   set_by_lua_block $variant {
#     math.randomseed(os.time())
#     if math.random() < 0.5 then
#       return "control"
#     else  
#       return "treatment"
#     end
#   }
#   add_header Set-Cookie "experiment_variant=$variant; Path=/; Max-Age=1209600";
# }
#
# SESSION-BASED ASSIGNMENT:
# - Assign based on session ID
# - Good for anonymous users
# - Consistent within session
#
# GEOGRAPHIC ASSIGNMENT:
# - Assign based on user location
# - Useful for region-specific tests
# - Good for infrastructure testing

# TESTING COMMANDS:
#
# 1. Deploy A/B test infrastructure:
# kubectl apply -f 01-basic-ab-testing.yaml
#
# 2. Verify traffic distribution:
# kubectl get pods -l app=webapp
# kubectl get endpoints webapp-service
#
# 3. Test control group directly:
# kubectl port-forward service/webapp-control-service 8081:80
# curl http://localhost:8081  # Should show control variant
#
# 4. Test treatment group directly:
# kubectl port-forward service/webapp-treatment-service 8082:80
# curl http://localhost:8082  # Should show treatment variant
#
# 5. Test user assignment:
# curl -H "X-User-ID: user123" http://myapp.company.com
# curl -H "X-User-ID: user456" http://myapp.company.com
# # Same user ID should always get same variant
#
# 6. Monitor experiment metrics:
# kubectl logs -l variant=control --tail=100 | grep conversion
# kubectl logs -l variant=treatment --tail=100 | grep conversion

# LOAD TESTING FOR STATISTICAL POWER:
#
# Generate traffic for both groups:
# kubectl run load-test-control --image=fortio/fortio --restart=Never -- \
#   load -qps 10 -t 3600s -c 5 \
#   -H "X-User-ID: control-user-{#}" \
#   http://webapp-service/
#
# kubectl run load-test-treatment --image=fortio/fortio --restart=Never -- \
#   load -qps 10 -t 3600s -c 5 \
#   -H "X-User-ID: treatment-user-{#}" \
#   http://webapp-service/

# ANALYTICS TRACKING:
#
# Each pod should track these events:
# {
#   "event_type": "page_view",
#   "user_id": "user123",
#   "experiment_id": "exp-homepage-001",
#   "variant": "control",
#   "timestamp": "2025-08-01T12:00:00Z",
#   "page": "/homepage",
#   "session_id": "sess_abc123"
# }
#
# {
#   "event_type": "button_click",
#   "user_id": "user123", 
#   "experiment_id": "exp-homepage-001",
#   "variant": "control",
#   "timestamp": "2025-08-01T12:01:30Z",
#   "button_id": "cta_signup",
#   "button_color": "blue"
# }
#
# {
#   "event_type": "conversion",
#   "user_id": "user123",
#   "experiment_id": "exp-homepage-001", 
#   "variant": "control",
#   "timestamp": "2025-08-01T12:05:15Z",
#   "conversion_type": "signup",
#   "value": 1
# }

# STATISTICAL ANALYSIS:
#
# Sample Size Calculation:
# - Baseline conversion rate: 3%
# - Minimum detectable effect: 5% (relative)
# - Statistical power: 80%
# - Confidence level: 95%
# - Required sample size: ~16,000 users per group
#
# A/B Test Results Analysis:
# Control Group:  
# - Users: 16,000
# - Conversions: 480 (3.0%)
# - Confidence Interval: [2.7%, 3.3%]
#
# Treatment Group:
# - Users: 16,000  
# - Conversions: 520 (3.25%)
# - Confidence Interval: [2.9%, 3.6%]
#
# Statistical Significance Test:
# - P-value: 0.032 (< 0.05, statistically significant)
# - Effect Size: +8.3% relative improvement
# - 95% CI of difference: [0.02%, 0.48%]

# EXPERIMENT CONCLUSION CRITERIA:
#
# DECLARE WINNER (Treatment) IF:
# ✅ Statistical significance achieved (p < 0.05)
# ✅ Practical significance achieved (effect > minimum detectable)
# ✅ Guardrail metrics not violated
# ✅ Sufficient sample size reached
# ✅ Experiment duration completed (min 1 week)
#
# DECLARE NO EFFECT IF:
# ⚖️ No statistical significance after sufficient sample size
# ⚖️ Effect size below minimum detectable effect
# ⚖️ Confidence interval includes zero
#
# STOP EXPERIMENT EARLY IF:
# ❌ Guardrail metrics violated (performance degradation)
# ❌ Significant negative impact detected
# ❌ Technical issues affecting results
# ❌ External factors invalidating results

# ROLLOUT STRATEGY AFTER EXPERIMENT:
#
# IF TREATMENT WINS:
# 1. Gradually rollout winning variant
# 2. Monitor for regression during rollout
# 3. Update baseline for future experiments
# 4. Document learnings and insights
#
# kubectl scale deployment webapp-control --replicas=0
# kubectl scale deployment webapp-treatment --replicas=10
# kubectl patch service webapp-service -p '{"spec":{"selector":{"variant":"treatment"}}}'
#
# IF NO SIGNIFICANT DIFFERENCE:
# 1. Keep current version (control)
# 2. Analyze why hypothesis was wrong
# 3. Design follow-up experiments
# 4. Consider other optimization approaches
#
# IF TREATMENT LOSES:
# 1. Stop experiment immediately
# 2. Keep current version (control)
# 3. Analyze negative results
# 4. Learn from failure for future tests