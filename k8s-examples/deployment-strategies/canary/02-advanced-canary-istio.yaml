# Advanced Canary Deployment with Istio Service Mesh
# Purpose: Production-grade canary deployments with fine-grained traffic control

# WHY: Need precise traffic control and advanced routing capabilities
# PROBLEM: Basic pod-based canary routing lacks fine control and observability
# SOLUTION: Use Istio service mesh for percentage-based traffic splitting and monitoring

# NOTE: This example requires Istio service mesh
# Install: istioctl install --set values.defaultRevision=default
#         kubectl label namespace default istio-injection=enabled

---
# STABLE VERSION DEPLOYMENT
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-stable
  labels:
    app: webapp
    version: stable
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
      version: stable
  template:
    metadata:
      labels:
        app: webapp
        version: stable
      annotations:
        sidecar.istio.io/inject: "true"    # Enable Istio sidecar injection
    spec:
      containers:
      - name: webapp
        image: nginx:1.27-alpine
        ports:
        - containerPort: 8080
          name: http
        
        env:
        - name: APP_VERSION
          value: "v1.0.0"
        - name: TRACK
          value: "stable"
        
        # Health checks
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
        
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"

---
# CANARY VERSION DEPLOYMENT
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-canary
  labels:
    app: webapp
    version: canary
spec:
  replicas: 3                              # Same as stable for capacity
  selector:
    matchLabels:
      app: webapp
      version: canary
  template:
    metadata:
      labels:
        app: webapp
        version: canary
      annotations:
        sidecar.istio.io/inject: "true"    # Enable Istio sidecar injection
    spec:
      containers:
      - name: webapp
        image: nginx:1.27-alpine           # New version image
        ports:
        - containerPort: 8080
          name: http
        
        env:
        - name: APP_VERSION
          value: "v2.0.0"                  # New version
        - name: TRACK
          value: "canary"
        - name: NEW_FEATURE_ENABLED
          value: "true"                    # New feature toggle
        
        # Identical health checks
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
        
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        
        # Identical resource allocation
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"

---
# KUBERNETES SERVICE (Both versions)
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  labels:
    app: webapp
spec:
  type: ClusterIP
  selector:
    app: webapp                            # Selects both stable and canary
  ports:
  - port: 80
    targetPort: 8080
    name: http

---
# ISTIO DESTINATION RULE (Define service subsets)
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: webapp-destination-rule
spec:
  host: webapp-service
  subsets:
  - name: stable
    labels:
      version: stable
    trafficPolicy:
      connectionPool:
        tcp:
          maxConnections: 100
        http:
          http1MaxPendingRequests: 10
          maxRequestsPerConnection: 2
  - name: canary
    labels:
      version: canary
    trafficPolicy:
      connectionPool:
        tcp:
          maxConnections: 100
        http:
          http1MaxPendingRequests: 10
          maxRequestsPerConnection: 2

---
# ISTIO VIRTUAL SERVICE (Traffic splitting configuration)
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: webapp-virtual-service
spec:
  hosts:
  - webapp-service
  - myapp.production.local               # External host
  gateways:
  - webapp-gateway
  - mesh                                 # For internal traffic
  http:
  # HEADER-BASED ROUTING (for testing)
  - match:
    - headers:
        x-canary-user:
          exact: "true"
    route:
    - destination:
        host: webapp-service
        subset: canary
      weight: 100
    fault:                               # Fault injection for testing
      delay:
        percentage:
          value: 0.1
        fixedDelay: 5s
  
  # PERCENTAGE-BASED TRAFFIC SPLITTING
  - route:
    - destination:
        host: webapp-service
        subset: stable
      weight: 90                         # 90% to stable
    - destination:
        host: webapp-service
        subset: canary
      weight: 10                         # 10% to canary
    
    # ADVANCED ROUTING FEATURES
    timeout: 30s
    retries:
      attempts: 3
      perTryTimeout: 10s
      retryOn: 5xx,reset,connect-failure,refused-stream
    
    # CIRCUIT BREAKER
    fault:
      abort:
        percentage:
          value: 0.01                    # 0.01% abort rate for testing
        httpStatus: 500

---
# ISTIO GATEWAY (External traffic entry point)
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: webapp-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - myapp.production.local
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: webapp-tls-secret
    hosts:
    - myapp.production.local

---
# SERVICE MONITOR FOR PROMETHEUS METRICS
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: webapp-canary-monitor
  labels:
    app: webapp
spec:
  selector:
    matchLabels:
      app: webapp
  endpoints:
  - port: http-monitoring
    interval: 15s
    path: /stats/prometheus            # Istio metrics endpoint

---
# CANARY ANALYSIS WITH FLAGGER (Optional - requires Flagger)
apiVersion: flagger.app/v1beta1
kind: Canary
metadata:
  name: webapp-canary-analysis
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp-canary
  
  # PROGRESSIVE TRAFFIC INCREASE
  service:
    port: 80
    targetPort: 8080
    gateways:
    - webapp-gateway
    hosts:
    - myapp.production.local
  
  # CANARY ANALYSIS CONFIGURATION
  analysis:
    interval: 1m                         # Check every minute
    threshold: 5                         # 5 successful checks to proceed
    maxWeight: 50                        # Max 50% traffic to canary
    stepWeight: 5                        # Increase by 5% each step
    
    # SUCCESS METRICS (must pass to continue)
    metrics:
    - name: request-success-rate
      templateRef:
        name: success-rate
        namespace: istio-system
      thresholdRange:
        min: 99                          # 99% success rate required
      interval: 1m
    
    - name: request-duration
      templateRef:
        name: latency
        namespace: istio-system
      thresholdRange:
        max: 500                         # Max 500ms latency
      interval: 1m
    
    # WEBHOOKS FOR NOTIFICATIONS
    webhooks:
    - name: load-test
      url: http://flagger-loadtester.test/
      metadata:
        type: bash
        cmd: "hey -z 1m -q 10 -c 2 http://webapp-service/"
    
    - name: slack-notification
      url: https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK
      metadata:
        type: slack
        channel: "#deployments"

# ISTIO CANARY DEPLOYMENT PROCESS:
#
# PHASE 1: INITIAL STATE (100% Stable)
# ┌─────────────────────────────────────────┐
# │              STABLE (v1.0)              │
# │    ┌─────┐    ┌─────┐    ┌─────┐       │
# │    │ Pod │    │ Pod │    │ Pod │       │
# │    └─────┘    └─────┘    └─────┘       │
# │              100% Traffic               │
# └─────────────────────────────────────────┘
#                      ▲
#              Istio Virtual Service
#                (weight: 100)
#
# PHASE 2: CANARY INTRODUCTION (90% Stable, 10% Canary)
# ┌─────────────────────────────────┐ ┌─────────────────┐
# │        STABLE (v1.0)            │ │   CANARY (v2.0) │
# │  ┌─────┐  ┌─────┐  ┌─────┐     │ │   ┌─────┐       │
# │  │ Pod │  │ Pod │  │ Pod │     │ │   │ Pod │       │
# │  └─────┘  └─────┘  └─────┘     │ │   └─────┘       │
# │        90% Traffic              │ │   10% Traffic   │
# └─────────────────────────────────┘ └─────────────────┘
#                      ▲                        ▲
#              Istio Virtual Service
#            (stable weight: 90, canary weight: 10)

# ISTIO TRAFFIC MANAGEMENT COMMANDS:
#
# 1. Deploy initial configuration:
# kubectl apply -f 02-advanced-canary-istio.yaml
#
# 2. Verify Istio injection:
# kubectl get pods -o jsonpath='{.items[*].spec.containers[*].name}' | grep istio-proxy
#
# 3. Check traffic distribution:
# kubectl get virtualservice webapp-virtual-service -o yaml
#
# 4. Test canary with header routing:
# curl -H "x-canary-user: true" http://myapp.production.local
#
# 5. PROGRESSION: Update traffic weights (50/50):
# kubectl patch virtualservice webapp-virtual-service --type='merge' -p='
# spec:
#   http:
#   - route:
#     - destination:
#         host: webapp-service
#         subset: stable
#       weight: 50
#     - destination:
#         host: webapp-service  
#         subset: canary
#       weight: 50'
#
# 6. Monitor with Istio dashboards:
# istioctl dashboard kiali
# istioctl dashboard grafana
# istioctl dashboard jaeger

# ADVANCED TRAFFIC CONTROL:
#
# User-based canary (route specific users to canary):
# - match:
#   - headers:
#       user-id:
#         regex: "user-(1[0-9]|[1-9])"  # Users 1-19 get canary
#   route:
#   - destination:
#       host: webapp-service
#       subset: canary
#
# Geographic canary (route specific regions):
# - match:
#   - headers:
#       x-region:
#         exact: "us-west-2"
#   route:
#   - destination:
#       host: webapp-service
#       subset: canary
#
# Time-based canary (route during specific hours):
# - match:
#   - headers:
#       x-time-slot:
#         regex: "(09|10|11|12|13|14|15|16|17):[0-9][0-9]"
#   route:
#   - destination:
#       host: webapp-service
#       subset: canary

# OBSERVABILITY AND MONITORING:
#
# View traffic metrics in Kiali:
# istioctl dashboard kiali
# # Navigate to Graph view to see traffic flow
#
# Check Istio proxy metrics:
# kubectl exec -it deployment/webapp-stable -c istio-proxy -- curl localhost:15000/stats/prometheus
#
# View distributed traces:
# istioctl dashboard jaeger
# # Search for traces by service name
#
# Monitor with Grafana:
# istioctl dashboard grafana
# # Use Istio service dashboards

# AUTOMATED PROMOTION WITH FLAGGER:
#
# Install Flagger:
# kubectl apply -k github.com/fluxcd/flagger//kustomize/istio
#
# Monitor canary progress:
# kubectl get canary webapp-canary-analysis --watch
# kubectl describe canary webapp-canary-analysis
#
# View Flagger events:
# kubectl get events --field-selector involvedObject.name=webapp-canary-analysis
#
# Manual promotion (skip analysis):
# kubectl annotate canary webapp-canary-analysis flagger.app/skip-analysis="true"
#
# Manual rollback:
# kubectl annotate canary webapp-canary-analysis flagger.app/skip-analysis="false"

# ROLLBACK STRATEGIES:
#
# Immediate rollback (set canary weight to 0):
# kubectl patch virtualservice webapp-virtual-service --type='merge' -p='
# spec:
#   http:
#   - route:
#     - destination:
#         host: webapp-service
#         subset: stable
#       weight: 100
#     - destination:
#         host: webapp-service
#         subset: canary
#       weight: 0'
#
# Gradual rollback (reduce canary traffic step by step):
# # 50% -> 25% -> 10% -> 0%
# kubectl patch virtualservice webapp-virtual-service --type='merge' -p='...'

# PRODUCTION MONITORING CHECKLIST:
#
# TRAFFIC METRICS:
# ☐ Success rate by version (>99% required)
# ☐ Latency percentiles by version (p50, p95, p99)
# ☐ Request volume by version
# ☐ Error rate by error type (4xx, 5xx)
#
# INFRASTRUCTURE METRICS:
# ☐ CPU utilization by version
# ☐ Memory usage by version  
# ☐ Network I/O by version
# ☐ Disk I/O by version
#
# BUSINESS METRICS:
# ☐ Conversion rates by version
# ☐ User engagement metrics
# ☐ Revenue impact analysis
# ☐ Feature adoption rates
#
# ALERTS CONFIGURATION:
# ☐ Error rate spike alerts
# ☐ Latency increase alerts
# ☐ Traffic distribution alerts
# ☐ Circuit breaker activation alerts

# TROUBLESHOOTING:
#
# Check Istio configuration:
# istioctl analyze
# istioctl proxy-config route deployment/webapp-stable
#
# Verify traffic distribution:
# istioctl proxy-config cluster deployment/webapp-stable --fqdn webapp-service.default.svc.cluster.local
#
# Debug connection issues:
# istioctl proxy-config listeners deployment/webapp-stable
# istioctl proxy-config endpoints deployment/webapp-stable
#
# Check Envoy logs:
# kubectl logs deployment/webapp-stable -c istio-proxy