# Real-World Health Check Scenarios and Troubleshooting
# WHY: Learn from common patterns and avoid typical mistakes
# WHAT: Production-ready examples with proper configuration

# =================== SCENARIO 1: E-COMMERCE WEB APPLICATION ===================

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ecommerce-web-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ecommerce-web-app
  template:
    metadata:
      labels:
        app: ecommerce-web-app
    spec:
      containers:
      - name: web-app
        image: nginx:1.21  # Replace with your e-commerce app
        ports:
        - containerPort: 3000
        env:
        - name: DATABASE_URL
          value: "postgresql://user:pass@db:5432/ecommerce"
        - name: REDIS_URL
          value: "redis://cache:6379"
        
        # Startup probe - Wait for app initialization
        startupProbe:
          httpGet:
            path: /health/startup
            port: 3000
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 24        # 2 minutes max startup
          successThreshold: 1
        
        # Liveness probe - Basic "am I alive" check
        livenessProbe:
          httpGet:
            path: /health/live
            port: 3000
            httpHeaders:
            - name: User-Agent
              value: "k8s-liveness-probe"
          initialDelaySeconds: 0
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        # Readiness probe - Can handle user traffic
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 3000
            httpHeaders:
            - name: User-Agent
              value: "k8s-readiness-probe"
          initialDelaySeconds: 0
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2

---
# Health check endpoints for e-commerce app:
# 
# GET /health/startup - Checks if app has finished initialization
# {
#   "status": "starting|ready",
#   "checks": {
#     "database_migration": "complete",
#     "config_loaded": "complete",
#     "cache_warmed": "complete"
#   }
# }
# 
# GET /health/live - Lightweight check for liveness
# {
#   "status": "alive",
#   "timestamp": "2024-01-15T10:30:00Z",
#   "uptime": "3600s"
# }
# 
# GET /health/ready - Comprehensive readiness check
# {
#   "status": "ready|not_ready",
#   "checks": {
#     "database": "ok",
#     "cache": "ok",
#     "payment_service": "ok",
#     "inventory_service": "ok"
#   }
# }

---
# =================== SCENARIO 2: MICROSERVICE WITH CIRCUIT BREAKER ===================

apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service
spec:
  replicas: 2
  selector:
    matchLabels:
      app: order-service
  template:
    metadata:
      labels:
        app: order-service
    spec:
      containers:
      - name: order-service
        image: nginx:1.21  # Replace with your order service
        ports:
        - containerPort: 8080
        env:
        - name: PAYMENT_SERVICE_URL
          value: "http://payment-service:8080"
        - name: INVENTORY_SERVICE_URL
          value: "http://inventory-service:8080"
        
        # Startup probe - Service initialization
        startupProbe:
          httpGet:
            path: /actuator/health/startup
            port: 8080
          initialDelaySeconds: 15
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 20
        
        # Liveness probe - Internal health (no external deps)
        livenessProbe:
          httpGet:
            path: /actuator/health/liveness
            port: 8080
          initialDelaySeconds: 0
          periodSeconds: 15
          timeoutSeconds: 5
          failureThreshold: 3
        
        # Readiness probe - Can handle requests (includes circuit breaker state)
        readinessProbe:
          httpGet:
            path: /actuator/health/readiness
            port: 8080
          initialDelaySeconds: 0
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2

---
# Circuit breaker readiness logic:
# 
# GET /actuator/health/readiness
# {
#   "status": "ready|not_ready",
#   "circuit_breakers": {
#     "payment_service": "closed",      # OK
#     "inventory_service": "half_open"  # Degraded but functional
#   },
#   "degraded_mode": false
# }
# 
# Returns 503 if:
# - All circuit breakers are open
# - Critical dependency is down
# - Service is in maintenance mode

---
# =================== SCENARIO 3: DATA PROCESSING BATCH JOB ===================

apiVersion: batch/v1
kind: Job
metadata:
  name: data-processing-job
spec:
  template:
    spec:
      containers:
      - name: data-processor
        image: nginx:1.21  # Replace with your data processing image
        env:
        - name: DATA_SOURCE
          value: "s3://data-bucket/input/"
        - name: OUTPUT_DEST
          value: "s3://data-bucket/output/"
        
        # Liveness probe - Prevent hanging jobs
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - |
              # Check if process is making progress
              current_time=$(date +%s)
              last_progress_file="/tmp/last_progress"
              
              if [ -f "$last_progress_file" ]; then
                last_progress=$(cat "$last_progress_file")
                time_diff=$((current_time - last_progress))
                
                # If no progress for 10 minutes, consider hung
                if [ $time_diff -gt 600 ]; then
                  echo "Process appears hung, no progress for ${time_diff}s"
                  exit 1
                fi
              fi
              
              echo "Process is making progress"
              exit 0
          initialDelaySeconds: 300    # Allow 5 minutes for initial setup
          periodSeconds: 60
          timeoutSeconds: 10
          failureThreshold: 3
        
        # Readiness probe - Not typically used for batch jobs
        # But can be used to signal when job is ready to start processing
        readinessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - |
              # Check if all input data is available
              if [ -f /tmp/input_data_ready ]; then
                echo "Input data is ready"
                exit 0
              else
                echo "Waiting for input data"
                exit 1
              fi
          initialDelaySeconds: 10
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 10
      restartPolicy: OnFailure

---
# =================== SCENARIO 4: STATEFUL DATABASE CLUSTER ===================

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres-cluster
spec:
  serviceName: postgres-cluster
  replicas: 3
  selector:
    matchLabels:
      app: postgres-cluster
  template:
    metadata:
      labels:
        app: postgres-cluster
    spec:
      containers:
      - name: postgres
        image: postgres:13
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_PASSWORD
          value: "clusterpassword"
        - name: POSTGRES_REPLICATION_MODE
          value: "master"
        - name: POSTGRES_REPLICATION_USER
          value: "replicator"
        - name: POSTGRES_REPLICATION_PASSWORD
          value: "replicatorpassword"
        
        # Startup probe - Database cluster initialization
        startupProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - |
              # Check if database is ready and cluster is initialized
              pg_isready -U postgres &&
              psql -U postgres -c "SELECT 1" > /dev/null 2>&1
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30        # 5 minutes for cluster setup
        
        # Liveness probe - Database process health
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - |
              # Check if postgres process is running and responsive
              pg_isready -U postgres -t 3
          initialDelaySeconds: 0
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        
        # Readiness probe - Can accept connections and handle queries
        readinessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - |
              # Check if database can handle queries
              pg_isready -U postgres -t 3 &&
              psql -U postgres -c "SELECT 1" > /dev/null 2>&1 &&
              # Check replication status if this is a replica
              if [ "$POSTGRES_REPLICATION_MODE" = "slave" ]; then
                psql -U postgres -c "SELECT pg_is_in_recovery()" | grep -q "t"
              fi
          initialDelaySeconds: 0
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 2

---
# =================== SCENARIO 5: NGINX REVERSE PROXY ===================

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-proxy
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-proxy
  template:
    metadata:
      labels:
        app: nginx-proxy
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
        - containerPort: 443
        
        # Mount custom nginx config
        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
        
        # Startup probe - Nginx started and config loaded
        startupProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 2
          timeoutSeconds: 1
          failureThreshold: 10
        
        # Liveness probe - Nginx is responding
        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 0
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 3
        
        # Readiness probe - Can proxy requests to backends
        readinessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - |
              # Test nginx config and backend connectivity
              nginx -t > /dev/null 2>&1 &&
              curl -f http://localhost/health > /dev/null 2>&1 &&
              # Check if backends are reachable
              curl -f --max-time 2 http://backend-service:8080/health > /dev/null 2>&1
          initialDelaySeconds: 0
          periodSeconds: 5
          timeoutSeconds: 5
          failureThreshold: 2
      
      volumes:
      - name: nginx-config
        configMap:
          name: nginx-config

---
# =================== COMMON TROUBLESHOOTING SCENARIOS ===================

# SCENARIO: Pods keep restarting due to liveness probe failures
# 
# Problem indicators:
# - High restart count in `kubectl get pods`
# - Liveness probe failures in `kubectl describe pod`
# - CrashLoopBackOff status
# 
# Common causes:
# 1. Liveness probe too aggressive (low timeouts/thresholds)
# 2. Application actually hanging/deadlocking
# 3. Resource constraints (CPU/memory limits too low)
# 4. Incorrect probe configuration (wrong port/path)
# 
# Debug steps:
# kubectl describe pod <pod-name>  # Check events
# kubectl logs <pod-name> --previous  # Check last logs before restart
# kubectl exec -it <pod-name> -- curl http://localhost:8080/health  # Test probe manually

---
# SCENARIO: Pods show as ready but return errors
# 
# Problem indicators:
# - Pods show 1/1 Ready but users get errors
# - Service endpoints include problematic pods
# - Application errors in logs
# 
# Common causes:
# 1. Readiness probe too simple (doesn't check dependencies)
# 2. Race conditions in health check endpoints
# 3. Readiness probe passes but app is actually degraded
# 
# Solutions:
# - Improve readiness probe to check all dependencies
# - Add dependency circuit breakers
# - Implement proper health check endpoints

---
# SCENARIO: Slow deployment rollouts
# 
# Problem indicators:
# - Deployments take very long to complete
# - Pods stuck in "Not Ready" state
# - New pods never become ready
# 
# Common causes:
# 1. Readiness probe failing on new version
# 2. Startup probe configured incorrectly
# 3. New version has bugs affecting health checks
# 
# Debug steps:
# kubectl get pods -w  # Watch pod status changes
# kubectl describe deployment <deployment-name>  # Check rollout status
# kubectl logs <pod-name>  # Check new pod logs

---
# =================== MONITORING AND ALERTING ===================

# Key metrics to monitor:
# 1. Probe failure rates
# 2. Pod restart counts
# 3. Service endpoint availability
# 4. Probe response times
# 5. Time to readiness for new pods

# Prometheus metrics examples:
# probe_success_total{probe_type="liveness"}
# probe_success_total{probe_type="readiness"}
# kube_pod_container_status_restarts_total
# kube_endpoint_address_available

# Alerting rules:
# - High liveness probe failure rate
# - Frequent pod restarts
# - Slow pod startup times
# - Service endpoints below threshold

---
# =================== BEST PRACTICES SUMMARY ===================

# 1. Design health check endpoints properly
#    - /health/live: Quick internal check
#    - /health/ready: Comprehensive dependency check
#    - /health/startup: Initialization status

# 2. Configure probes appropriately
#    - Startup: Generous timeouts for slow apps
#    - Liveness: Conservative, internal checks only
#    - Readiness: Comprehensive, includes dependencies

# 3. Monitor probe behavior
#    - Set up alerts for probe failures
#    - Monitor probe response times
#    - Track deployment rollout success

# 4. Test probe endpoints
#    - Unit test health check logic
#    - Integration test with dependencies
#    - Load test probe performance

# 5. Handle edge cases
#    - Graceful degradation
#    - Circuit breaker patterns
#    - Maintenance mode handling